{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0637) tensor(0.9876)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 222
    }
   ],
   "source": [
    "m = 100\n",
    "# 生成一个包含有m个样本的数据集，每个样本有2个特征，1个标签\n",
    "# 1. 生成特征数据，这应该是一个 m * 2 的matrix\n",
    "X = torch.randn((m, 2)) # 每个值都是从 均值为 0 ，方差为 1 的标准正态分布中采样获得, 可以输出X.mean()和X.std()观察得到, randn应是 rand normal\n",
    "print(X.mean(), X.std())\n",
    "\n",
    "# 2. 生成标签数据，这应该是一个 m * 1 的vector, 我们一半值取1， 一半值取0\n",
    "y = torch.cat([torch.zeros(int(m / 2),dtype=torch.int32), torch.ones(int(m / 2))], axis=0)\n",
    "y\n",
    "# 这里X 以及 1向量 和 0向量 cat 的 y 都是tensor， 因为我并没有指定它们的dtype(data type)为torch.float32 or torch.float64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "e-01, -1.1182e+00],\n",
       "        [ 1.0000e+00,  1.4779e+00, -1.3806e+00],\n",
       "        [ 1.0000e+00, -1.0055e+00,  2.3039e-01],\n",
       "        [ 1.0000e+00, -3.1007e-01, -8.6014e-02],\n",
       "        [ 1.0000e+00, -2.5664e-01,  8.7401e-01],\n",
       "        [ 1.0000e+00, -1.7063e+00,  9.5552e-01],\n",
       "        [ 1.0000e+00,  3.2069e-01,  1.3826e+00],\n",
       "        [ 1.0000e+00, -1.2527e+00,  5.9950e-01],\n",
       "        [ 1.0000e+00, -7.3443e-01, -4.0536e-01],\n",
       "        [ 1.0000e+00,  6.3664e-01, -4.4485e-01],\n",
       "        [ 1.0000e+00, -1.4488e+00,  1.7712e+00],\n",
       "        [ 1.0000e+00,  2.4880e-01, -1.4894e+00],\n",
       "        [ 1.0000e+00, -1.0759e+00,  1.4520e+00],\n",
       "        [ 1.0000e+00,  7.5844e-01, -1.9787e-01],\n",
       "        [ 1.0000e+00, -9.0210e-01,  7.5137e-01],\n",
       "        [ 1.0000e+00,  9.7429e-01, -1.3163e-01],\n",
       "        [ 1.0000e+00, -1.1640e+00,  7.7195e-01],\n",
       "        [ 1.0000e+00,  6.3517e-01,  4.8320e-01],\n",
       "        [ 1.0000e+00,  6.1315e-01,  1.1784e+00],\n",
       "        [ 1.0000e+00, -4.1494e-01,  2.4502e-01],\n",
       "        [ 1.0000e+00,  7.4884e-01,  2.7826e-01]])"
      ]
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": [
    "# 处理输入样本特征，为特征添加一个值为 1 的特征. 注意torch.ones参数要是 m, 1 否则它没有axis=1来和 X 进行concate\n",
    "X = torch.cat([torch.ones(m,1), X], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 224
    }
   ],
   "source": [
    "# 初始化模型参数theta, [theta_0, theta_1, theta_2]\n",
    "theta = torch.zeros(3, requires_grad=True)# 我们后面构造的计算图需要对它求梯度用来更新参数\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + torch.exp(z))\n",
    "def h(theta, X):\n",
    "    return sigmoid(torch.mv(X, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造计算loss的计算图，对于loss，我们只要写出它与输入X之间关系的表达式就行\n",
    "def get_loss(theta, X, y):\n",
    "    loss = - torch.dot(y, torch.log(h(theta, X))) + torch.dot((1-y), torch.log(h(theta, X))).sum()\n",
    "    return loss \n",
    "# 设定学习率\n",
    "alpha = 0.01\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([  0.0000, -12.1060,   5.4159])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([  0.0000, -12.1060,   5.4159])"
      ]
     },
     "metadata": {},
     "execution_count": 227
    }
   ],
   "source": [
    "loss = get_loss(theta, X, y)\n",
    "loss.backward(retain_graph=True)\n",
    "print(theta.grad)\n",
    "# theta.grad.zero_()\n",
    "theta.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-50a80747af4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# print(grad)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 参数theta更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtheta\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# 计算损失并记录\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "# 记录损失值得变化\n",
    "losses = []\n",
    "for _ in range(epochs):\n",
    "    # theta梯度置为0\n",
    "    theta.grad.zero_()\n",
    "    loss.backward()\n",
    "    grad = theta.grad\n",
    "    # print(grad)\n",
    "    # 参数theta更新, 在pytorch中这样更新参数改变叶子变量的值是不可以的，pytoch有对应的函数来对其进行参数更新\n",
    "    theta -= alpha * grad\n",
    "    # 计算损失并记录\n",
    "    loss = get_loss(theta, X, y)\n",
    "    losses.append(loss)\n",
    "theta, losses"
   ]
  }
 ]
}